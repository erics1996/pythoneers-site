<!DOCTYPE html>
<html lang="en">
    <head>

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Erics">
        <link rel="canonical" href="https://pythoneers.cn/web-crawler/scrapy.html">
        <link rel="shortcut icon" href="../img/favicon.ico">
<!--sougou-->
<meta name="keywords"
      content="python入门,python学习,python文档,python教程,python,django,flask,python web,python爬虫,python人工智能,python数据分析,python深度学习,python量化交易"/>
<meta http-equiv="Content-Type" content="text/html;charset=gb2312"/>
<meta name="sogou_site_verification" content="atcH4bxi4f"/>
<!--!sougou-->

        <title>Scrapy - Python开发栈</title>
        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <link href="../css/nav.css" rel="stylesheet">
        <link href="../css/public.css" rel="stylesheet">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="..">Python开发栈</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Python <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../python/python-basis.html">Python基础入门</a>
</li>
                                    
<li >
    <a href="../python/python-advance.html">Python进阶</a>
</li>
                                    
<li >
    <a href="../python/python-senior.html">Python高级</a>
</li>
                                    
<li >
    <a href="../python/module.html">内置模块与第三方模块</a>
</li>
                                    
<li >
    <a href="../python/algorithm.html">数据结构与算法</a>
</li>
                                    
<li >
    <a href="../python/design-pattern.html">设计模式</a>
</li>
                                    
<li >
    <a href="../python/interviews-python.html">Python面试题</a>
</li>
                                    
<li >
    <a href="../python/interviews-network.html">网络相关面试题</a>
</li>
                                    
<li >
    <a href="../python/nowcoder-sword-offer.html">剑指Offer</a>
</li>
                                    
<li >
    <a href="../python/leetcode-algorithm.html">Leetcode算法</a>
</li>
                                    
<li >
    <a href="../python/leetcode-db.html">Leetcode数据库</a>
</li>
                                    
<li >
    <a href="../python/leetcode-multithreading.html">Leetcode多线程</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Python Web <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../python-web/django.html">Django</a>
</li>
                                    
<li >
    <a href="../python-web/drf.html">Django Rest Framework</a>
</li>
                                    
<li >
    <a href="../python-web/flask.html">Flask</a>
</li>
                                    
<li >
    <a href="../python-web/celery.html">Celery</a>
</li>
                                    
<li >
    <a href="../python-web/mysql.html">MySQL</a>
</li>
                                    
<li >
    <a href="../python-web/interviews-flask.html">Flask面试题</a>
</li>
                                    
<li >
    <a href="../python-web/interviews-django.html">Django面试题</a>
</li>
                                    
<li >
    <a href="../python-web/interviews-mysql.html">MySQL面试题</a>
</li>
                                    
<li >
    <a href="../python-web/interviews-redis.html">Redis面试题</a>
</li>
                                    
<li >
    <a href="../python-web/project-flask.html">Flask项目实战</a>
</li>
                                    
<li >
    <a href="../python-web/project-django.html">Django项目实战</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Python GUI <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../gui/pyqt5.html">PyQt5</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">网络爬虫 <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="scrapy.html">Scrapy</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">数据分析 <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../data-analysis/numpy.html">NumPy</a>
</li>
                                    
<li >
    <a href="../data-analysis/pandas.html">Pandas</a>
</li>
                                    
<li >
    <a href="../data-analysis/matplotlib.html">Matplotlib</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">计算机视觉 <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../computer-vision/opencv.html">OpenCV</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">机器学习 <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../machine-learning/quantitative-trading.html">机器学习与量化交易</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">深度学习 <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../deep-learning/tensorflow.html">TensorFlow</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                            <li >
                                <a rel="next" href="../gui/pyqt5.html">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../data-analysis/numpy.html">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#_1">爬虫基本原理概述</a></li>
            <li><a href="#1">1. 爬虫基本流程</a></li>
            <li><a href="#2">2. 请求与响应</a></li>
            <li><a href="#3">3. 请求</a></li>
            <li><a href="#4">4. 响应</a></li>
            <li><a href="#5">5. 抓取内容数据类型</a></li>
            <li><a href="#6">6. 解析内容的方式</a></li>
            <li><a href="#7">7. 数据的保存</a></li>
        <li class="main "><a href="#scrapy">scrapy的安装</a></li>
            <li><a href="#1-windows">1. Windows下的安装</a></li>
            <li><a href="#2-linux">2. Linux下的安装</a></li>
        <li class="main "><a href="#scrapy_1">scrapy项目的创建</a></li>
            <li><a href="#1-scrapy">1. 创建scrapy项目</a></li>
            <li><a href="#2_1">2. 创建爬虫应用</a></li>
            <li><a href="#3_1">3. 启动爬虫项目</a></li>
        <li class="main "><a href="#scrapy_2">scrapy详解</a></li>
            <li><a href="#1-scrapy_1">1. scrapy框架功能</a></li>
            <li><a href="#2-scrapy">2. scrapy的整体架构</a></li>
            <li><a href="#3_2">3. 组件执行流程</a></li>
            <li><a href="#4-twisted">4. twisted</a></li>
            <li><a href="#5_1">5. 响应与解析</a></li>
            <li><a href="#6-pipeline">6. pipeline持久化</a></li>
            <li><a href="#7-dupefilter">7. dupefilter去重</a></li>
            <li><a href="#8">8. 深度限制</a></li>
            <li><a href="#9-cookie">9. 处理cookie</a></li>
            <li><a href="#10">10. 起始请求定制</a></li>
            <li><a href="#11">11. 下载中间件</a></li>
            <li><a href="#12">12. 爬虫中间件</a></li>
            <li><a href="#13">13. 定制命令</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p><img alt="scrapy.jpeg" src="../img/web-crawler/scrapy.jpeg" /></p>
<h4 id="_1">爬虫基本原理概述</h4>
<hr>

<h5 id="1">1. 爬虫基本流程</h5>
<p>爬虫基本工作流程是：发起请求、获取响应内容、解析响应内容、保存数据</p>
<p>( 1 ) 发起请求：通过HTTP库向目标站点发起请求，即发送一个 Request，请求可以包含额外的headers等信息，等待服务器响应。</p>
<p>( 2 ) 获取响应内容：如果服务器能正常响应，会得到一个 Response，Response 的内容便是所要获取的页面内容，<font>类型可能有 HTML，Json 字符串，二进制数据（如图片视频）等类型</font>。</p>
<p>( 3 ) 解析响应内容：得到的内容 <font>可能是 HTML，可以用正则表达式、网页解析库进行解析</font>。<font>可能是Json，可以直接转为 Json 对象解析</font>，可能是二进制数据，可以做保存或者进一步的处理。</p>
<p>( 4 ) 保存数据：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。</p>
<hr>

<h5 id="2">2. 请求与响应</h5>
<p>浏览器发送消息给该网址所在的服务器，这个过程叫做 <font>HTTP Request</font> 请求。服务器收到浏览器发送的消息后，能够根据浏览器发送消息的内容，做相应处理，然后把消息回传给浏览器。这个过程叫做 <font>HTTP Response</font> 响应。 浏览器收到服务器的 Response 信息后，会对信息进行相应处理，然后展示。</p>
<hr>

<h5 id="3">3. 请求</h5>
<p>( 1 ) 请求方式：<font>主要有GET、POST两种类型</font>，另外还有 HEAD、PUT、DELETE、OPTIONS 等</p>
<p>( 2 ) 请求 URL：URL 全称统一资源定位符，如一个网页文档、一张图片、一个视频等都可以用 URL 唯一来确定</p>
<p>( 3 ) 请求头：包含请求时的头部信息，如 User-Agent、Host、Cookies 等信息</p>
<p>( 4 ) 请求体：请求时额外携带的数据，如表单提交时的表单数据</p>
<hr>

<h5 id="4">4. 响应</h5>
<p>( 1 ) 响应状态：有多种响应状态，如 200 代表成功、301 跳转、404 找不到页面、502服务器错误</p>
<p>( 2 ) 响应头：如内容类型、内容长度、服务器信息、设置 Cookie 等等</p>
<p>( 3 ) 响应体：最主要的部分，包含了请求资源的内容，如网页 HTML、图片二进制数据等</p>
<hr>

<h5 id="5">5. 抓取内容数据类型</h5>
<p>( 1 ) 网页文本：如 HTML 文档、Json 格式文本等</p>
<p>( 2 ) 图片：获取到的是二进制文件，保存为图片格式</p>
<p>( 3 ) 视频：同为二进制文件，保存为视频格式即可</p>
<p>( 4 ) 其他数据：只要是能请求到的，一般都能获取</p>
<hr>

<h5 id="6">6. 解析内容的方式</h5>
<p>直接处理、Json 解析、正则表达式、bs4、PyQuery 和 Path 等方式。如果遇到js渲染问题可以通过 <font>分析 Ajax 请求、Selenium/WebDriver</font>、Splash、PyV8和Ghost等来处理。</p>
<hr>

<h5 id="7">7. 数据的保存</h5>
<p>( 1 ) 文本：纯文本、Json、Xml 等</p>
<p>( 2 ) 关系型数据库：如 MySQL、Oracle、SQL Server 等具有结构化表结构形式存储</p>
<p>( 3 ) 非关系型数据库：如 MongoDB、Redis 等 Key-Value 形式存储</p>
<p>( 4 ) 二进制文件：如图片、视频、音频等等直接保存成特定格式即可</p>
<hr>

<h4 id="scrapy">scrapy的安装</h4>
<p><font>scrapy 框架是一个大而全的爬虫框架</font>，有了 scrapy 框架，beautifulsoup 和 requests 模块都用不上了，因为 scrapy 框架包含了这些模块。
<hr></p>
<h5 id="1-windows">1. Windows下的安装</h5>
<p>在 Windows 系统中，使用 <strong><code>$ pip3 install scrapy</code></strong> 命令可以安装 scrapy。但是如果遇到 Twisted 模块不能被安装，就需要单独下载该模块。如果遇到 Twisted 模块安装不上，</p>
<p>( 1 ) 下载 python 版本对应的 Twisted 模块，根据 python 的位数和版本选择：<strong><a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/">https://www.lfd.uci.edu/~gohlke/pythonlibs/</a></strong></p>
<p>( 2 ) 安装该 Twisted 模块</p>
<p>直接使用 <strong><code>$ pip install Twisted‑19.7.0‑cp38‑cp38m‑win_amd64.whl</code></strong>是不能安装的。pip 只支持从网络上的包管理库中找到这个模块下载下来。本地安装这个 .whl 文件是不支持的，需要先安装 wheel 模块：<strong><code>$ pip install wheel</code></strong>。然后，再安装本地 whl 文件：<strong><code>$ pip install Twisted‑19.7.0‑cp38‑cp38m‑win_amd64.whl</code></strong></p>
<p>( 3 ) 安装 windows 中独有的 pywin32 模块：<strong><code>$ pip install pywin32</code></strong></p>
<p>( 4 ) 安装 scrapy 模块：<strong><code>$ pip3 install scrapy</code></strong></p>
<p>注意：截至目前，使用 <strong><code>$ pip3 install scrapy</code></strong> 是可以把所有依赖安装成功的，也就是说 Twisted 模块不再需要单独安装。另外，记得单独安装 pywin32 模块。
<hr></p>
<h5 id="2-linux">2. Linux下的安装</h5>
<p>直接执行 <strong><code>pip3 install scrapy</code></strong> 命令 scrapy 框架依赖的所有模块和组件都安装了。
<hr></p>
<h4 id="scrapy_1">scrapy项目的创建</h4>
<hr>

<h5 id="1-scrapy">1. 创建scrapy项目</h5>
<p>创建 scrapy 项目的命令：<strong><code>scrapy startproject 项目名称</code></strong></p>
<p>执行这条指令后，会自动创建一个 scrapy_demo 目录，scrapy_demo 目录中有以下目录/文件：</p>
<pre><code>├── scrapy.cfg 
└── scrapy_demo
    ├── __init__.py
    ├── items.py
    ├── middlewares.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
</code></pre>

<p><strong><code>settings.py</code></strong> ：也是配置文件（爬虫）</p>
<p><strong><code>scrapy.cfg</code></strong> ：是主配置文件（上线部署）</p>
<p><strong><code>spiders</code></strong> ：爬虫文件</p>
<p><strong><code>middlewares.py</code></strong> ：中间件（当请求到来时统一处理）</p>
<p><strong><code>items.py</code></strong> ：持久化（写在硬盘上、数据库、redis）</p>
<p><strong><code>pipelines.py</code></strong> ：持久化
<hr></p>
<h5 id="2_1">2. 创建爬虫应用</h5>
<pre><code>cd 项目名称
scrapy genspider chouti chouti.com
</code></pre>

<pre><code class="linux">├── scrapy.cfg 
└── scrapy_demo
    ├── __init__.py
    ├── items.py
    ├── middlewares.py
    ├── pipelines.py
    ├── settings.py
    └── spiders
        ├── chouti.py
        ├── cnblogs.py
        ├── __init__.py
</code></pre>

<hr>

<h5 id="3_1">3. 启动爬虫项目</h5>
<pre><code>scrapy crawl chouti
scrapy crawl cnblogs
</code></pre>

<p><font>注意：默认情况下，只能启动一个爬虫。</font></p>
<hr>

<h4 id="scrapy_2">scrapy详解</h4>
<hr>

<h5 id="1-scrapy_1">1. scrapy框架功能</h5>
<p>( 1 ) 使用 twisted 下载页面</p>
<p>( 2 ) HTML 解析对象</p>
<p>( 3 ) 支持代理</p>
<p>( 4 ) 延迟下载</p>
<p>( 5 ) 去重</p>
<p>( 6 ) 深度和广度等</p>
<hr>

<h5 id="2-scrapy">2. scrapy的整体架构</h5>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20190616085632703.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RoYW5sb24=,size_16,color_FFFFFF,t_70" /></p>
<hr>

<h5 id="3_2">3. 组件执行流程</h5>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20200509123401861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RoYW5sb24=,size_16,color_FFFFFF,t_70" /></p>
<p>( 1 ) 引擎找到要执行的爬虫（scrapy crawl xxx会找到找到xxx爬虫名），执行爬虫的 start_requests 方法，并得到一个迭代器</p>
<p>( 2 ) 迭代器循环时会获取 request 对象，而 request 对象中封装了要访问的 url 和回调函数</p>
<p>( 3 ) 将所有 request 对象（任务）放到调度器中，用于以后被下载器下载</p>
<p>( 4 ) 下载器去调度器中获取要下载任务(就是 request 对象)，下载完成后执行回调函数</p>
<p>( 5 ) 回到 spider 的回调函数中（如果 yield Request() 是重新放到调度器中去，如果 yield Item() 对象会到 pipeline）</p>
<hr>

<h5 id="4-twisted">4. twisted</h5>
<p>scrapy 框架依赖于 twisted，twisted 是一个基于事件循环的异步非阻塞模块（简而言之，一个线程同时可以向多个目标发送 http 请求）。其内部是基于事件循环机制来实现爬虫的并发。scrapy 在下载页面的时候使用的就是 twisted，所以我们至少得明白 twisted 的原理。</p>
<p>下面的例子使用的是 requests 模块，性能会低一些：</p>
<pre><code class="py">import requests

url_request = ['https://www.blueflags.cn/', 'https://segmentfault.com/', 'https://stackoverflow.com/']
for item in url_request:
    '''
    一个一个串行地等待
    '''
    response = requests.get(item)
    print(response.text)
</code></pre>

<p>下面是 twisted 的例子：</p>
<pre><code class="py">'''
teisted:是一个基于事件循环的异步非阻塞模块，简而言之：也就是一个线程同时可以向多个目标发送http请求
1、阻塞：不等待，请求发出去不用等待结果，而是继续法下一个连接请求。也就是发送一个连接，马上发下一个连接请求。
import socket
sk = socket.socket()
sk.setblocking(False)
sk.connect((1.2.3.4,80))

import socket
sk = socket.socket()
sk.setblocking(False)
sk.connect((1.2.3.4,80))
……
2、异步：体现在回调，拿到结果后才会（执行回调）通知请求者
回调函数：
def callback(contents):
    print(contents)
3、事件循环：循环检测，循环三个socket任务，检查其请求者的状态，是否连接成功，是否返回结果。
'''
from twisted.web.client import getPage, defer
from twisted.internet import reactor

# 第一部分：代理开始接收任务
def callback(contents):  # content:kiku
    pass

deferred_list = []  # 三个任务[(thanlon,kiku),(thanlon,yuqin)]
url_list = ['https://www.blueflags.cn/', 'https://segmentfault.com/', 'https://stackoverflow.com/']
for url in url_list:
    deferred = getPage(bytes(url, encoding='utf-8'))
    deferred.addCallback(callback)  # 如果找到了，就告诉请求过来取
    deferred_list.append(deferred)
# 代理执行完任务后停止
dlist = defer.DeferredList(deferred_list)

def all_done(arg):
    reactor.stop()

dlist.addBoth(all_done)  # 不管成功与否，我都会停下来
# 第三部分：代理开始处理
reactor.run()
</code></pre>

<p>( 1 ) twisted 非阻塞体现在：不等待，请求发出去不用等待结果，而是继续法下一个连接请求。也就是发送一个连接，马上发下一个连接请求</p>
<p>( 2 ) twisted 的异步体现在：拿到结果后才会（执行回调）通知请求者</p>
<p>( 3 ) twisted事件循环体现在：循环检测，循环三个 socket 任务，检查其请求者的状态，是否连接成功，是否返回结果</p>
<blockquote>
<p><font>twisted 相对于 requests 并发效率提高了。</font></p>
</blockquote>
<hr>

<h5 id="5_1">5. 响应与解析</h5>
<p>( 1 ) 响应：request.text，request.encoding，request.body，request.request（当前响应是由哪个请求发起，请求中封装了要访问的 url 以及下载完成后执行哪个函数）</p>
<p>( 2 ) 解析（xpath）</p>
<table>
<thead>
<tr>
<th>xpath</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr>
<td>//</td>
<td>子子孙孙里面找</td>
</tr>
<tr>
<td>/</td>
<td>找儿子</td>
</tr>
<tr>
<td>.//</td>
<td>当前往下找子子孙孙</td>
</tr>
<tr>
<td>//div[@属性名='']</td>
<td></td>
</tr>
<tr>
<td>//div/text</td>
<td>取文本</td>
</tr>
<tr>
<td>//div/@href</td>
<td>取属性</td>
</tr>
<tr>
<td>.extract()</td>
<td>获取所有结果</td>
</tr>
<tr>
<td>.extract_first()</td>
<td>获取第一条结果</td>
</tr>
</tbody>
</table>
<pre><code class="py">tag_list = response.xpath(&quot;//div[@href='x']/a&quot;)
for tag in tag_list:
    tag.xpath('.//p/text()').extract_first()
</code></pre>

<p>( 3 ) 再次发请求</p>
<pre><code class="py">yield Request(url='xxx',callback=self.parse) # 只是把url和callback封装在一起，没有发起请求
</code></pre>

<hr>

<h5 id="6-pipeline">6. pipeline持久化</h5>
<p>对于本文第10部分 “ scrapy 爬取网页简单示例（猫眼电影）” 是存在缺点的，具体有如下两点：</p>
<p>( 1 ) 爬虫刚开始时，无法完成打开连接；爬虫关闭时，无法完成关闭连接</p>
<p>( 2 ) parse函数中应该是只做解析的，持久化部分应该交给另外的组件，也就是说分工是不明确</p>
<p>对于以上两个缺点，scrapy 也有对应的解决方案，可以使用 scrapy 中 pipeline 与 items。具体操作如下：</p>
<p>① 先写 pipeline 类</p>
<p><strong><code>pipelines.py</code></strong> ：</p>
<pre><code class="py">class ScrapyPipelinePipeline(object):  # 类名不是写死的，可自定义
    def process_item(self, item, spider):
        return item
</code></pre>

<p>② 写 Item 类</p>
<p><strong><code>items.py</code></strong> ：</p>
<pre><code class="py">class ScrapyPipelineItem(scrapy.Item):  # 类名不是写死的，可自定义
    href = scrapy.Field()
    title = scrapy.Field()
</code></pre>

<p>③ 配置</p>
<p><strong><code>settings.py</code></strong> ：</p>
<pre><code class="py">ITEM_PIPELINES = {
   'scrapy_pipeline.pipelines.ScrapyPipelinePipeline': 300,
}
</code></pre>

<p>④ 爬虫</p>
<pre><code class="py">yield Item对象
</code></pre>

<p>只要 yield Item 对象，就会自动执行 pipelines.py 中的 <kbd>process_item</kbd> 方法。执行 process_item 方法的时候就可以帮助我们做操作了。yield 执行一次 process_item 方法就会被调用一次。</p>
<p>以上四部分操作是可以解决分工不明确的缺点，对于第一个问题可以这样解决：</p>
<p><strong><code>pipelines.py</code></strong>：</p>
<pre><code class="py">class ScrapyPipelinePipeline(object):  # 类名不是写死的，可自定义
    '''
    方法执行顺序:__init__、open_spider、process_item、close_spider方法
    '''
    def __init__(self):
        self.f = None

    def open_spider(self, spider):
        '''
        爬虫开始执行时调用
        :param spider:
        :return:
        '''
        self.f = open('data.log', 'a')

    def process_item(self, item, spider):
        self.f.write(item['href'] + '\n')
        return item

    def close_spider(self, spider):
        '''
        爬虫关闭时被调用
        :param spider:
        :return:
        '''
        self.f.close()
</code></pre>

<p>将文件的路径写到配置文件 settings.py 中（pipeline 的五个方法）：</p>
<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py">HREF_FILE_PATH = 'data.log'
</code></pre>

<p><strong><code>pipeline.py</code></strong>：</p>
<pre><code class="py"># from scrapy_pipeline.settings import HREF_FILE_PATH#一般不导入文件

class ScrapyPipelinePipeline(object):  # 类名不是写死的，可自定义
    '''
    源码内容：
    第一步：
        判断当前ScrapyPipelinePipeline类中是否含有from_crawler，如果有
        obj = ScrapyPipelinePipeline.from_crawler(……)
        如果没有：
        obj = ScrapyPipelinePipeline()
    第二步：
        obj.open_spider(self, spider)
    第三步：
        obj.process_item(self, item, spider)、obj.process_item()，obj.process_item()……
    第四步：
        obj.close_spider(self, spider)
    '''
    def __init__(self, path):
        self.f = None
        self.path = path

    @classmethod
    def from_crawler(cls, crawler):  # cls是当前类
        '''
        初始化的时候用于创建pipeline对象(不用实例化就可以创建)
        :param crawler:
        :return:
        '''
        path = crawler.settings.get('HREF_FILE_PATH')  # 向所有的配置文件(自己的配置和内置的配置)找HREF_FILE_PATH
        return cls(path)

    def open_spider(self, spider):
        '''
        爬虫开始执行时调用
        :param spider:
        :return:
        '''
        self.f = open(self.path, 'a')

    def process_item(self, item, spider):
        self.f.write(item['href'] + '\n')
        return item

    def close_spider(self, spider):
        '''
        爬虫关闭时被调用
        :param spider:
        :return:
        '''
        self.f.close()
</code></pre>

<p>注意：Pipeline 类中最多有5个方法，在执行 process_item 时，返回的 item 的值交给 pipeline 的 process_item 方法。</p>
<p>如果要求持久化到数据库，再写一个 Pipeline 即可：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
# from scrapy_pipeline.settings import HREF_FILE_PATH#一般不导入文件

class FilePipeline(object):  # 类名不是写死的，可自定义
    '''
    源码内容：
    第一步：
        判断当前ScrapyPipelinePipeline类中是否含有from_crawler，如果有
        obj = ScrapyPipelinePipeline.from_crawler(……)
        如果没有：
        obj = ScrapyPipelinePipeline()
    第二步：
        obj.open_spider(self, spider)
    第三步：
        obj.process_item(self, item, spider)、obj.process_item()，obj.process_item()……
    第四步：
        obj.close_spider(self, spider)
    '''
    def __init__(self, path):
        self.f = None
        self.path = path

    @classmethod
    def from_crawler(cls, crawler):  # cls是当前类
        '''
        初始化的时候用于创建pipeline对象(不用实例化就可以创建)
        :param crawler:
        :return:
        '''
        print('File.from_crawler')
        path = crawler.settings.get('HREF_FILE_PATH')  # 向所有的配置文件(自己的配置和内置的配置)找HREF_FILE_PATH
        return cls(path)

    def open_spider(self, spider):
        '''
        爬虫开始执行时调用
        :param spider:
        :return:
        '''
        print('File.open_spider')
        self.f = open(self.path, 'a+')

    def process_item(self, item, spider):
        print('file', item['href'])
        self.f.write(item['href'] + '\n')
        # return item  # 交给下一个pipeline的process_item方法
        # raise DropItem()

    def close_spider(self, spider):
        '''
        爬虫关闭时被调用
        :param spider:
        :return:
        '''
        print('File.close_spider')
        self.f.close()

class DbPipeline(object):  # 类名不是写死的，可自定义
    def __init__(self, path):
        self.f = None
        self.path = path

    @classmethod
    def from_crawler(cls, crawler):  # cls是当前类
        '''
        初始化的时候用于创建pipeline对象(不用实例化就可以创建)
        :param crawler:
        :return:
        '''
        print('Db.from_crawler')
        path = crawler.settings.get('HREF_DB_PATH')  # 向所有的配置文件(自己的配置和内置的配置)找HREF_FILE_PATH
        return cls(path)

    def open_spider(self, spider):
        '''
        爬虫开始执行时调用
        :param spider:
        :return:
        '''
        print('Db.open_spider')
        self.f = open(self.path, 'a')

    def process_item(self, item, spider):
        print('db', item['href'])
        self.f.write(item['href'] + '\n')
        return item

    def close_spider(self, spider):
        '''
        爬虫关闭时被调用
        :param spider:
        :return:
        '''
        print('Db.close_spider')
        self.f.close()
</code></pre>

<p>两个类的方法执行顺序是：(先实例化类)</p>
<pre><code>File.from_crawler
File.from_crawler
Db.open_spider
Db.open_spider
File.process_item（在执行process_item的时候，先写到文件中，再写到数据库中）
File.process_item
File.process_item
File.process_item
db.close_spider
db.close_spider
</code></pre>

<p>如果不想让下一个 pipeline 运行，那么第一个 pipeline 的 <font>process_spider</font> 方法就不要 return item，这个时候我们可以使用异常。注意：第二个 pipeline 的其它方法还是可以执行的。</p>
<pre><code class="py">from scrapy.exceptions import DropItem
……
    def process_item(self, item, spider):
        print('file', item['href'])
        self.f.write(item['href'] + '\n')
        # return item  # 交给下一个pipeline的process_item方法
        raise DropItem()
</code></pre>

<p><font>pipeline 是所有爬虫公用的，如果想要给某个爬虫定制需要使用 spider 参数自己进行处理</font>。下面是，使用 spider 参数的例子：</p>
<pre><code class="py"> def open_spider(self, spider):
        '''
        爬虫开始执行时调用
        :param spider:
        :return:
        '''
        if spider.name=='cnblogs':
            print('File.open_spider')
        self.f = open(self.path, 'a+')
</code></pre>

<hr>

<h5 id="7-dupefilter">7. dupefilter去重</h5>
<p>( 1 ) 默认的去重规则</p>
<p>对于访问过的就不再请求了。因为在执行 <strong><code>yield Request(…)</code></strong> 的时候，会先执行 <kbd>request_seen</kbd> 方法：</p>
<p><strong><code>dupefilter.py</code></strong>：</p>
<pre><code class="py">def request_seen(self, request):
    fp = self.request_fingerprint(request)#将url(request)转换成md5值，fp就是url的md5值
    if fp in self.fingerprints:#fingerprints是一个集合，不能重复
        return True#表示已经访问过了
    self.fingerprints.add(fp) # 未访问过，把fp加进集合了
   if self.file:#如果有文件也会把访问记录写到文件中
   self.file.write(fp + os.linesep)
</code></pre>

<p>系统的 dupefilter，但是一般公司不用。</p>
<p>( 2 ) 自定义去重规则</p>
<p>① 修改默认的去重规则：</p>
<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py">DUPEFILTER_CLASS = 'scrapy_dupefilter.dupefilters.MyDupeFilter'
</code></pre>

<p>② 初步自定义去重规则</p>
<p><strong><code>dupefilters.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.dupefilters import BaseDupeFilter

class MyDupeFilter(BaseDupeFilter):
    def __init__(self):
        self.visted_url = set()

    @classmethod
    def from_settings(cls, settings):
        '''
        内部会先检查有没有from_settings，如果有可以通过cls()拿到对象，如果没有自己实例化
        :param settings:
        :return:
        '''
        return cls()

    def request_seen(self, request):
        '''
        :param request:
        :return:# None表示都没有访问过,会再次访问,也就是没有去重
        '''
        if request.url in self.visted_url:  # 如果在访问过的集合中
            return True  #
        self.visted_url.add(request.url)

    def open(self):  # can return deferred
        pass

    def close(self, reason):  # can return a deferred
        pass
</code></pre>

<p>③ 唯一标识的处理</p>
<p>但是用 url 做唯一标识不好，因为如果是效果一样的 url 可能被当作不同的 url，如：</p>
<pre><code class="py">url = 'https://www.blueflags.cn?k1=123&amp;k2=456'
url2 = 'https://www.blueflags.cn?k1=456&amp;k2=123'
</code></pre>

<p>如何处理相同效果的 url：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.utils.request import request_fingerprint
from scrapy.http import Request

url = 'https://www.blueflags.cn?k1=123&amp;k2=456'
req = Request(url=url)  # 用Request做封装
url2 = 'https://www.blueflags.cn?k1=456&amp;k2=123'
req2 = Request(url=url)
fd = request_fingerprint(request=req)
fd2 = request_fingerprint(request=req2)
print(fd)
print(fd2)
'''
c54f8702bcb7065bfb901f509fd6de6e60d93db2 
c54f8702bcb7065bfb901f509fd6de6e60d93db2
'''
</code></pre>

<p>④ 自定义去重规则的完善</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.dupefilters import BaseDupeFilter
from scrapy.utils.request import request_fingerprint

class MyDupeFilter(BaseDupeFilter):
    def __init__(self):
        self.visted_fd = set()#这里只是放到集合中，其实是可以放到redis中的

    @classmethod
    def from_settings(cls, settings):
        '''
        内部会先检查有没有from_settings，如果有可以通过cls()拿到对象，如果没有自己实例化
        :param settings:
        :return:
        '''
        return cls()

    def request_seen(self, request):
        '''
        :param request:
        :return:# None表示都没有访问过,会再次访问,也就是没有去重
        '''
        fd = request_fingerprint(request=request)
        if fd in self.visted_fd:  # 如果在访问过的集合中
            return True
        self.visted_fd.add(fd)

    def open(self):  # can return deferred
        print('爬虫开始的时候')

    def close(self, reason):  # can return a deferred
        print('爬虫开始的时候')

    # def log(self, request, spider):  # 记录日志的
    #     print('日志')
</code></pre>

<p><font>注意：如果想要遵循去重规则，需要满足两点：在 request_seen 中编写正确的逻辑 dont_filter = False</font></p>
<hr>

<h5 id="8">8. 深度限制</h5>
<p>( 1 ) 查看 depth</p>
<pre><code class="py">import scrapy

class CnblogsSpider(scrapy.Spider):
    name = 'cnblogs'
    allowed_domains = ['cnblogs.com']
    start_urls = ['http://cnblogs.com/']

    def parse(self, response):
        print(response.request.url, response.meta)
        '''
         response.meta = {
            'download_timeout': 180.0,
            'download_slot': 'cnblogs.com',
            'download_latency': 0.07253193855285645,
            'redirect_times': 1,
            'redirect_ttl': 19,
            'redirect_urls': ['http://cnblogs.com/'],
            'redirect_reasons': [301]
        }
        '''
        print(response.request.url, response.meta.get('depth'))  # meta中是没有depth,所以或取到的是None
</code></pre>

<p>( 2 ) 深度限制</p>
<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py"># 限制深度
DEPTH_LIMIT = 3 # 只能访问3层以内
</code></pre>

<hr>

<h5 id="9-cookie">9. 处理cookie</h5>
<p>( 1 ) 拼接字典</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from urllib.parse import urlencode

d = {
    'k1': 'v1',
    'k2': 'v2'
}
ret = urlencode(d)
print(ret)  # k1=v1&amp;k2=v2
</code></pre>

<p>( 2 ) 处理 cookie 方式一</p>
<p>① 携带 cookie</p>
<pre><code class="py">yield Request(
    url='https://dig.chouti.com/login',
    method='POST',
    body='password=123456&amp;loginType=2&amp;phone=%2B8618512152005',
    # body=urlencode({
    #     'password': 2232323232,
    #     'loginType': 2,
    #     'phone': +8618512152001
    # }),
    cookies=self.cookie_dict,
    headers={
        'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'
    },
    callback=self.check_login
)
</code></pre>

<p>② 解析 cookie</p>
<pre><code class="py"># 去响应头中获取cookie,cookie保存在cookie_jar对象
cookie_jar = CookieJar()
cookie_jar.extract_cookies(response, response.request)
for i, v in cookie_jar._cookies.items():
    print(i, v)
# 去对象中将cookie解析到字典
for k, v in cookie_jar._cookies.items():
    for i, j in v.items():
        for m, n in j.items():
            self.cookie_dict[m] = n.value
            # print(cookie_dict)
</code></pre>

<p>处理 cookie 的所有代码：</p>
<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py">USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
</code></pre>

<p><strong><code>chouti.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
import scrapy
from scrapy.http.cookies import CookieJar
from scrapy.http import Request
from urllib.parse import urlencode

class ChoutiSpider(scrapy.Spider):
    name = 'chouti'
    allowed_domains = ['dig.chouti.com']
    start_urls = ['https://dig.chouti.com']
    cookie_dict = {}

    def parse(self, response):
        print(response)
        '''
              第一次访问抽屉的时候返回的内容response
              :param response:
              :return:
        '''
        '''
        去响应头中获取cookie
        '''

        # 去响应头中获取cookie,cookie保存在cookie_jar对象
        cookie_jar = CookieJar()
        cookie_jar.extract_cookies(response, response.request)
        for i, v in cookie_jar._cookies.items():
            print(i, v)
        # 去对象中将cookie解析到字典
        for k, v in cookie_jar._cookies.items():
            for i, j in v.items():
                for m, n in j.items():
                    self.cookie_dict[m] = n.value
        # print(cookie_dict)
        yield Request(
            url='https://dig.chouti.com/login',
            method='POST',
            body='password=123456&amp;loginType=2&amp;phone=%2B8618512152005&amp;NECaptchaValidate=54qajoVtRONV-Pbu87QdaDAK2XjYVi-HdWYP8emDquaYO7N1Felra4dWFa5uzV6J0tHE5u74PMCmpKrBxJgwhsObcoT1ufv5BkbSub6f4kLxXfUx5WdSxTjZ7K2YPFR1HSpIQnWVr6OXq2fWdJ7C.MeVCvGscNXkX-t4DHKp8gbGtT9SZKA07zefpl101iRG0hmPNK.xBQViXsGPZAS2f.vLWVa8PXPbZ.Y-.uwGfzXCcF5F8UfT8vcIZF7Co1kJ9Bgo-82mLyyt4v6sFfWjUx_NvChvngrSy1ujoHAFTreiXNswonNkcNfCWMOGHRzAxMjGDqtzgdMmf1e55NWCRiAw-dZwSue.ERa8MXQeHIGtnQ-lJASGlWA6JolrmWGEovLrG.Ctrpk6d40xLk0EETzOb067taEoHvZGnLi2Y4tPc00qgA0iKYbKVQp7YbMSP2OKcxI-zReaEwDAVCNKLuBj7fUCP04Sb4B1ozK_gnmAj7i72AE4KlskWPS3',
            # body=urlencode({
            #     'password': 2232323232,
            #     'loginType': 2,
            #     'phone': +8618512152001
            # }),
            cookies=self.cookie_dict,
            headers={
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'
            },
            callback=self.check_login
        )

    def check_login(self, response):
        '''
        登录成功后的回调函数
        :param response:
        :return:
        '''
        print(response.text)
        yield Request(
            url='https://dig.chouti.com/',
            cookies=self.cookie_dict,
            callback=self.index
        )

    def index(self, response):
        news_list = response.xpath('//div[@id=&quot;content-list&quot;]/div[@class=&quot;item&quot;]')
        for new in news_list:
            link_id = new.xpath('.//div[@class=&quot;part2&quot;]/@share-linked').extract_first()
            yield Request(
                url='http://dig.chouti.com/link/vote?linksId=%s' % (link_id),
                method='POST',
                cookies=self.cookie_dict,
                callback=self.check_result
            )
        page_list = response.xpath('//div[@id=&quot;dig_lcpage&quot;]//a/@href').extract()
        for page in page_list:
            page = 'https://dig.chouti.com' + page
            yield Request(url=page, callback=self.index)  # 翻页点赞

    def check_result(self, response):
        print(response.text)
</code></pre>

<p>( 3 ) 处理 cookie 方式二</p>
<p>加上 meta，在 meta 中写上：</p>
<pre><code class="py">cookie_jar = True
</code></pre>

<hr>

<h5 id="10">10. 起始请求定制</h5>
<p>( 1 ) 内部原理</p>
<p>在爬虫中第一步没有执行 <kbd>parse</kbd> 方法，而是读取 start_urls，把每个 url 都封装成一个 request 对象，先执行的是 <kbd>start_requests</kbd> 方法。然后把 request 对象交给调度器。</p>
<p>( 2 ) scrapy引擎到爬虫中取起始url的步骤</p>
<p>① 调用 start_requests 并获取返回值</p>
<p>② v = iter(返回值)</p>
<p>③ req = 执行 <kbd>v.__next__()</kbd>，req2 = 执行 <kbd>v.__next__()</kbd>，req3 = 执行 <kbd>v.__next__()</kbd>。</p>
<p>④ 请求对象 request 全部放到调度器中</p>
<p>( 3 ) 可迭代器转换为可迭代对象</p>
<pre><code class="py"> # 可迭代对象转换为迭代器
l = [1, 2, 3]
print(iter(l))  # &lt;list_iterator object at 0x7f9f9fa1b358&gt;
</code></pre>

<pre><code class="py"># -*- coding: utf-8 -*-
'''
def func():
    yield 1
    yield 2
    yield 3

l = func()  # l是生成器，生成器既是生成器也是迭代器
# 让生成器变成迭代器，不再是生成器
print(iter(l))  # &lt;generator object func at 0x7f64037d46d8&gt;

v = l.__next__()
print(v)
'''

def get_all(arg):
    data = iter(arg)
    print(data.__next__())
</code></pre>

<p>( 4 ) 代码实现</p>
<pre><code class="py"> class ChoutiSpider(scrapy.Spider):
    name = 'chouti'
    allowed_domains = ['dig.chouti.com']
    start_urls = ['https://dig.chouti.com']
    cookie_dict = {}

    # 引擎第一步就是调用start_requests获取起始url
    # 如果我们写了优先级最高
    def start_requests(self):
        '''
        # 第一种方式
        # 引擎调用start_requests方法得到的是生成器,得到生成器后一个个拿到request,内部调用__next__方法
         for url in self.start_urls:
            yield Request(url=url)
        '''
        '''
        #第二种方式
        '''
        for url in self.start_urls:
            req_list = []
            req_list.append(Request(url=url))
            return req_list
</code></pre>

<p>( 5 ) 定制：可以去 redis 中获取</p>
<hr>

<h5 id="11">11. 下载中间件</h5>
<p>在下载的过程中，可以对请求和响应子定制一些操作。</p>
<p>( 1 ) 中间件的执行顺序</p>
<p>自定义中间件：</p>
<p><strong><code>mds.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
class Mds1(object):
    def process_request(self, request, spider):
        print('Mds1.process_request', request)
        return None

    def process_response(self, request, response, spider):
        print('Mds1.process_response',response)
        return response

class Mds2(object):
    def process_request(self, request, spider):
        print('Mds2.process_request', request)
        return None

    def process_response(self, request, response, spider):
        print('Mds2.process_response', request,response)
        return response
</code></pre>

<p>使用自定义的中间件还需要修改配置文件：</p>
<p><strong><code>seetting.py</code></strong>：</p>
<pre><code class="py">DOWNLOADER_MIDDLEWARES = {
    # 'scrapy_middleware.middlewares.ScrapyMiddlewareDownloaderMiddleware': 543,
    # 分值小的先执行
    'scrapy_middleware.mds.Mds1': 1,
    'scrapy_middleware.mds.Mds2': 2,
}
</code></pre>

<p><strong><code>qikeyishu.py</code></strong>：</p>
<pre><code class="py">import scrapy
from scrapy.http.request import Request

class QikeyishuSpider(scrapy.Spider):
    name = 'qikeyishu'
    allowed_domains = ['cnblogs.com']
    start_urls = (
        'https://www.cnblogs.com/',
    )

    def start_requests(self):
        '''
        自己定制起始url,如果没有写,url会使用默认的start_urls
        :return:
        '''
        url = 'https://www.cnblogs.com/qikeyishu/'
        yield Request(url=url, callback=self.parse, )

    def parse(self, response):
        print('response', response)  # response在中间件类执行process_response方法的时候有的
</code></pre>

<pre><code>thanlon@vivobook:~/PycharmProjects/scrapy_middleware$ scrapy crawl qikeyishu --nolog
Mds1.process_request &lt;GET https://www.cnblogs.com/qikeyishu/&gt;
Mds2.process_request &lt;GET https://www.cnblogs.com/qikeyishu/&gt;
Mds1.process_request &lt;GET https://www.cnblogs.com/robots.txt&gt;
Mds2.process_request &lt;GET https://www.cnblogs.com/robots.txt&gt;
Mds2.process_response &lt;GET https://www.cnblogs.com/robots.txt&gt; &lt;200 https://www.cnblogs.com/robots.txt&gt;
Mds1.process_response &lt;200 https://www.cnblogs.com/robots.txt&gt;
Mds2.process_response &lt;GET https://www.cnblogs.com/qikeyishu/&gt; &lt;200 https://www.cnblogs.com/qikeyishu/&gt;
Mds1.process_response &lt;200 https://www.cnblogs.com/qikeyishu/&gt;
response &lt;200 https://www.cnblogs.com/qikeyishu/&gt;
</code></pre>

<p>( 2 ) 中间件截获下载的任务，伪造结果，不去中间件下载</p>
<pre><code class="py">    def process_request(self, request, spider):
        print('Mds1.process_request', request)
        # return HtmlResponse(url='www.xxx.com', status=200, headers=None, body=b'thanlon')
</code></pre>

<p>( 3 ) 中间件截获下载的任务，不去中间件下载，自己下载</p>
<pre><code class="py">    def process_request(self, request, spider):
        print('Mds1.process_request', request)
        # return HtmlResponse(url='www.xxx.com', status=200, headers=None, body=b'thanlon')
        import requests
        ret = request.get(request.url)  # ret.content是字节
        return HtmlResponse(url=request.url, status=200, headers=None, body=ret.content)  # 告诉去请求的地址下载了,但结果是自己伪造的
</code></pre>

<p>( 4 ) 中间件返回 Request 对象</p>
<pre><code class="py">    def process_request(self, request, spider):
        print('Mds1.process_request', request)
        '''
        1. 返回Response
        '''
        # return HtmlResponse(url='www.xxx.com', status=200, headers=None, body=b'thanlon')
        import requests
        # ret = request.get(request.url)  # ret.content是字节
        # return HtmlResponse(url=request.url, status=200, headers=None, body=ret.content)  # 告诉去请求的地址下载了,但结果是自己伪造的
        '''
        2. 返回request
        '''
        # return Request('https://www.cnblogs.com/qikeyishu/')#自己一个请求自己
        '''
</code></pre>

<p>( 5 ) 中间件把请求丢掉，抛出异常</p>
<pre><code class="py">     def process_request(self, request, spider):
        print('Mds1.process_request', request)
        '''
        1. 返回Response
        '''
        # return HtmlResponse(url='www.xxx.com', status=200, headers=None, body=b'thanlon')
        import requests
        # ret = request.get(request.url)  # ret.content是字节
        # return HtmlResponse(url=request.url, status=200, headers=None, body=ret.content)  # 告诉去请求的地址下载了,但结果是自己伪造的
        '''
        2. 返回request
        '''
        # return Request('https://www.cnblogs.com/qikeyishu/')#自己一个请求自己
        '''
        3. 抛出异常,把请求丢弃掉,可以抛出异常
        '''
        from scrapy.exceptions import IgnoreRequest
        raise IgnoreRequest
</code></pre>

<p><font>中间件中的这三种情况都不做，一般我们做的是对请求信息的加工。</font></p>
<p>( 6 ) 对请求信息的加工</p>
<p>scrapy 默认是有对请求信息加工的，在 scrapy 框架的 usesragent.py 模块中：</p>
<p><strong><code>useragent.py</code></strong>：</p>
<pre><code class="py">class UserAgentMiddleware(object):
    &quot;&quot;&quot;This middleware allows spiders to override the user_agent&quot;&quot;&quot;

    def __init__(self, user_agent='Scrapy'):
        self.user_agent = user_agent

    @classmethod
    def from_crawler(cls, crawler):
        o = cls(crawler.settings['USER_AGENT'])
        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
        return o

    def spider_opened(self, spider):
        self.user_agent = getattr(spider, 'user_agent', self.user_agent)

    def process_request(self, request, spider):
        if self.user_agent:
            request.headers.setdefault(b'User-Agent', self.user_agent)
</code></pre>

<p>加载了配置文件 settings.py 文件中定义的 USER_AGENT，所以我们可以通过修改配置文件，来达到定义请求头的目的。</p>
<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py"># USER_AGENT = 'scrapy_middleware (+http://www.yourdomain.com)'
USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Safari/537.36'
</code></pre>

<p><strong><code>mds.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.http import HtmlResponse, Request

class Mds1(object):
    def process_request(self, request, spider):
        print('Mds1.process_request', request)
        '''
        1. 返回Response
        '''
        # return HtmlResponse(url='www.xxx.com', status=200, headers=None, body=b'thanlon')
        import requests
        # ret = request.get(request.url)  # ret.content是字节
        # return HtmlResponse(url=request.url, status=200, headers=None, body=ret.content)  # 告诉去请求的地址下载了,但结果是自己伪造的
        '''
        2. 返回request
        '''
        # return Request('https://www.cnblogs.com/qikeyishu/')#自己一个请求自己
        '''
        3. 抛出异常,把请求丢弃掉,可以抛出异常
        '''
        # from scrapy.exceptions import IgnoreRequest
        # raise IgnoreRequest
        '''
        4. 对请求进行加工
        '''
        request.hraders[
            'USER_AGENT'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.75 Safari/537.36'

    def process_response(self, request, response, spider):
        print('Mds1.process_response', response)
        return response

    def process_exception(self, request, exception, spider):
        pass

class Mds2(object):
    def process_request(self, request, spider):
        print('Mds2.process_request', request)
        return None

    def process_response(self, request, response, spider):
        print('Mds2.process_response', request, response)
        return response

    def process_exception(self, request, exception, spider):
        pass
</code></pre>

<p>( 7 ) 使用 from_crawler 类方法创建对象</p>
<p><strong><code>mds.py</code></strong>：</p>
<pre><code class="py">class Mds1(object):
    @classmethod
    def from_crawler(cls, crawler):
        '''
        如果有这个方法可以自己创建对象，如果没有可以自己创建对象
        :param crawler:
        :return:
        '''
        # This method is used by Scrapy to create your spiders.
        s = cls()
        return s
</code></pre>

<p>( 8 ) 应用：user-agent、代理</p>
<hr>

<h5 id="12">12. 爬虫中间件</h5>
<p>( 1 ) process_start_requests方法</p>
<p>process_start_requests 是 <font>用来读取起始返回的生成器</font>，然后一个一个再返回。把生成器 start_requests 方法返回的 <strong><code>yield Request(url=url, callback=self.parse, )</code></strong> 传到爬虫中间件类的 process_start_requests 方法的参数 start_requests 中。在 process_start_requests 方法中又返回生成器。</p>
<p><font>process_start_requests方法只在爬虫启动时执行一次。对于两个爬虫中间件，起始url交给第一个中间件循环一遍再交给第二个中间件循环一遍。</font></p>
<p>( 2 ) process_spider_input 和 process_spider_output</p>
<p>刚开始进来执行 process_start_requests，把起始 url 拿到后放到调度器中。调度器拿着 url 要去下载器下载，下载完成后， 经过下载中间件，然后到了引擎。引擎会把结果交给爬虫中间件，执行爬虫中间件的 <kbd>process_spider_input</kbd> 方法。执行完该方法后执行回调函数，回调函数可以 yield item 还可以 yield Request。当 yield item 或 yield Request 之后，会执行爬虫中间件中所有的 <kbd>process_spider_output</kbd> 方法，执行完 process_spider_output 方法后到引擎中。引擎会拿去重规则，放到调度器中或者放到 item pipeline。</p>
<p>( 3 ) 爬虫中间件的实现</p>
<p><strong><code>mds.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
class Sd1(object):
    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        return s

    def process_spider_input(self, response, spider):
        '''
        :param response:下载器帮助我们下载下来的结果，是经过所有下载中间件的process_response,
        然后交给下一个process_spider_input。都执行完成后，交给回调函数。回调函数可以yield item
        或yield Request,当yield之后就会调用process_spider_output
        :param spider:
        :return:
        '''
        return None

    def process_spider_output(self, response, result, spider):
        '''
        所以process_spider_output有两个参数
        :param response:上一次下载的结果
        :param result:yield新的Request()对象
        :param spider:
        :return:
        '''
        for i in result:  # yield item或yield Request之后还可以再yield一个个返回,间接一个个返回
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request, dict
        # or Item objects.
        pass

    # 只在爬虫启动时执行一次
    def process_start_requests(self, start_requests, spider):
        # Must return only requests (not items).
        for r in start_requests:
            yield r

class Sd2(object):

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, dict or Item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        pass

    def process_start_requests(self, start_requests, spider):
        for r in start_requests:
            yield r
</code></pre>

<p><strong><code>settings.py</code></strong>：</p>
<pre><code class="py">SPIDER_MIDDLEWARES = {
    # 'scrapy_middleware.middlewares.ScrapyMiddlewareSpiderMiddleware': 543,
    'scrapy_middleware.sd.Sd1': 1,
    'scrapy_middleware.sd.Sd2': 2,
}
</code></pre>

<p>( 4 ) 爬虫中间件的应用：深度、优先级。</p>
<hr>

<h5 id="13">13. 定制命令</h5>
<p>( 1 ) 单爬虫运行</p>
<p><strong><code>start.py</code></strong></p>
<pre><code class="py">from scrapy.cmdline import execute

if __name__ == '__main__':
    execute(['scrapy', 'crawl', 'qikeyishu', '--nolog'])
</code></pre>

<p>( 2 ) 多爬虫运行（单线程运行多个爬虫）</p>
<p>① 在 spider 同级目录创建一个目录，目录名可以自定义，我这里用 commands</p>
<p><img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20190917183617360.png" /></p>
<p>② 在 commands 目录下创建 crawlall.py 文件</p>
<p><strong><code>crawlall.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.commands import ScrapyCommand

class Command(ScrapyCommand):
    requires_project = True

    def syntax(self):
        '''
        语法支持,命令的提示,scrapy crawlall --help
        :return:
        '''
        return '[options]'

    def short_desc(self):
        '''
        命令的介绍,可通过crapy --help
        :return:
        '''
        return 'Run all of the spiders'

    def run(self, args, opts):
        spider_list = self.crawler_process.spiders.list()  # 找到所有爬虫
        # print(spider_list)
        for name in spider_list:  # 找到爬虫后循环这两个爬虫
            self.crawler_process.crawl(name, **opts.__dict__)  # 添加两个爬虫任务
        self.crawler_process.start()
</code></pre>

<p>③ 在 <strong><code>settings.py</code></strong> 中添加配置 <strong><code>COMMANDS_MODULE='项目名称.目录名称'</code></strong></p>
<pre><code class="py"># 自定制命令目录
COMMANDS_MODULE = 'scrapy_make_command.commands'
</code></pre>

<p>④ 在项目目录执行命令：<strong><code>scrapy crawlall</code></strong></p>
<p>想要执行 start.py 脚本也去执行所有爬虫的，可以这样：</p>
<p><strong><code>start.py</code></strong>：</p>
<pre><code class="py"># -*- coding: utf-8 -*-
from scrapy.cmdline import execute

if __name__ == '__main__':
    # execute(['scrapy', 'crawl', 'qikeyishu', '--nolog'])
    execute(['scrapy', 'crawlall', '--nolog'])
</code></pre>

<p><hr>
<div style="width: 60px;height: auto;z-index: 100;bottom: 30%;position: fixed;right: 0px" id="plug-ins">
    <div style="position: relative;float: right">
        <a target="" href="javascript:;" id="weibo"
           style="display: block;width: 40px;height: 40px;background-color: #c4351b;margin-top: 1px;">
            <img width="22" height="20" src="../img/weibo.png" alt=""
                 style="margin-top: 10px;margin-left: 9px">
        </a>
        <a target="_blank" href="http://sighttp.qq.com/authd?IDKEY=5838160dbeb2a49f264d5e2d13d6336248d74a60cf56ecad" id="qq" style="display: block;width: 40px;height: 40px;background-color:#0e91e8;margin-top: 1px">
            <img width="20" height="20" src="../img/qq.png" 
                 style="margin-top: 10px;margin-left: 10px" alt="点击这里给我发消息" title="点击这里给我发消息">
        </a>
        <a href="javascript:" id="wechat"
           style="display: block;width: 40px;height: 40px;background-color:#01b901;margin-top:1px">
            <img width="22" height="20" src="../img/wechat.png"
                 style="margin-top: 10px;margin-left: 9px">
        </a>
        <a href="javascript:" id="go_top"
           style="display: none;width: 40px;height: 40px;background-color: #b5b5b5;margin-top: 1px">
            <img width="22" height="20" src="../img/top.png" alt=""
                 style="margin-top: 10px;margin-left: 9px">
        </a>
    </div>
</div>
<!--双11 start-->
<div style="z-index: 100;position: fixed;left: 0;bottom: 0;" id="ads" hidden="hidden">
        <div>
            <button type="button" class="close" style="position: absolute;right: 5px;top: 0;font-size: 28px;opacity: 1;color: white"><span aria-hidden="true">&times;</span></button>
             <a target="_blank" href="https://s.click.taobao.com/1pElJvu">
                <img style="margin: 0;border-radius: unset" class="img-responsive" width="400" height="" src="img/ads/tianmap-800x450-1.jpg"
                    alt="2020天猫双11—联盟主会场（带超级红包）" title="2020天猫双11—联盟主会场（带超级红包）">
            </a><br>
            <a target="_blank" href="https://s.click.taobao.com/5EtkJvu">
                <img style="margin: 0;border-radius: unset" class="img-responsive" width="400" height="" src="img/ads/tianmap-800x450-2.jpg"
                    alt="2020天猫双11—联盟主会场（带超级红包）" title="2020天猫双11—联盟主会场（带超级红包）">
            </a>
        </div>
</div>
<!--双11 stop-->
<!--右侧广告 start-->
<div style="width: auto;height: auto;z-index: 99;position: fixed;right: 0;top: 70px;" id="google_ads">
        <div>
            <div style="width: 180px;height: auto"></div>
            <!-- Vertical -->
            <ins class="adsbygoogle"
                 style="display:block"
                 data-ad-client="ca-pub-6937898095875663"
                 data-ad-slot="2927491642"
                 data-ad-format="auto"
                 data-full-width-responsive="true"></ins>
        </div>
</div>
<!--右侧广告 stop--></p></div>
        </div>

        <footer class="col-md-12">
<!--    <hr>-->
<!---->
<!--以前的-->
<!--<p>Copyright © 2020-2022 <a href="https://pythoneers.cn/">pythoneers.cn</a> All Rights Reserved.</p>-->
<!--<p>-->
<!--    <a href="https://beian.miit.gov.cn" target="_blank" style="color:#555;">豫ICP备19037971号-2</a><a target="_blank"></a>-->
<!--    <script type="text/javascript">document.write(unescape("%3Cspan id='cnzz_stat_icon_1279280889'%3E%3C/span%3E%3Cscript src='https://s9.cnzz.com/z_stat.php%3Fid%3D1279280889%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>-->
<!--</p>-->
<!--<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>-->
<!--以前的/-->
<div class="container-fluid" style="background:#2C2C2C;color: #c1c1c1;font-weight: bold;margin-bottom: 2px">
    <div class="row">
        <div class="col-md-12  text-center" style="padding: 40px">
            <a href="https://tongji.baidu.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">百度统计</a>&nbsp;|
            <a href="https://ziyuan.baidu.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">百度搜索</a>&nbsp;|
            <a href="https://www.umeng.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">友盟</a>&nbsp;|
            <a href="https://www.tmall.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">天猫</a>&nbsp;|
            <a href="https://www.alimama.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">阿里妈妈</a>&nbsp;|
            <a href="https://blog.csdn.net/Thanlon" style="color:#c1c1c1;font-weight: bold" target="_blank">CSDN</a>&nbsp;|
            <a href="https://segmentfault.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">SegmentFault</a>&nbsp;|
            <!--        <a href="https://bysj39.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">39毕设网</a>&nbsp;|-->
            <a href="https://d5video.pythoneers.cn/" style="color:#c1c1c1;font-weight: bold" target="_blank">D5影视网</a>&nbsp;|
            <a href="https://ywyz.pythoneers.cn/" style="color:#c1c1c1;font-weight: bold" target="_blank">运维云栈</a>&nbsp;|
            <a href="https://www.mkdocs.org/" style="color:#c1c1c1;font-weight: bold" target="_blank">MkDocs</a>&nbsp;
            <!--        <a href="javascript:" style="color:#c1c1c1;font-weight: bold" target="_blank">β在线学习网</a>&nbsp;-->
            <!--          <a href="javascript:" style="color:#c1c1c1;font-weight: bold" target="_blank">阿尔法搜索</a>&nbsp;|-->
            <!--          <a href="javascript:" style="color:#c1c1c1;font-weight: bold" target="_blank">贝塔学习网</a>&nbsp;|-->
            <!--          <a href="javascript:" style="color:#c1c1c1;font-weight: bold" target="_blank">德尔塔操作系统</a>&nbsp;|-->
            <!--          <a href="javascript:" style="color:#c1c1c1;font-weight: bold" target="_blank">欧米伽影视</a>&nbsp;-->
            <br>
            <a href="https://docs.djangoproject.com/zh-hans/3.0/" style="color:#c1c1c1;font-weight: bold"
               target="_blank">Django</a>&nbsp;|
            <a href="https://flask.palletsprojects.com/en/1.1.x/" style="color:#c1c1c1;font-weight: bold"
               target="_blank">Flask</a>&nbsp;|
            <a href="https://www.django-rest-framework.org/" style="color:#c1c1c1;font-weight: bold"
               target="_blank">DRF</a>&nbsp;|
            <a href="https://cn.vuejs.org/" style="color:#c1c1c1;font-weight: bold" target="_blank">Vue</a>&nbsp;|
            <a href="https://www.nginx.com/" style="color:#c1c1c1;font-weight: bold" target="_blank">Nginx</a>&nbsp;|
            <a href="https://kubernetes.io/" style="color:#c1c1c1;font-weight: bold" target="_blank">Kubernetes</a>&nbsp;|
            <a href="https://zabbix.org/wiki/Main_Page" style="color:#c1c1c1;font-weight: bold"
               target="_blank">Zabbix</a>&nbsp;|
            <a href="https://tensorflow.google.cn/" style="color:#c1c1c1;font-weight: bold"
               target="_blank">TensorFlow</a>&nbsp;|
            <a href="https://docs.opencv.org/" style="color:#c1c1c1;font-weight: bold" target="_blank">OpenCV</a>&nbsp;
            <br>
            Copyright © 2020-2021&nbsp; pythoneers.cn. All Rights Reserved.&nbsp;Python开发栈 版权所有&nbsp;
            <br>
            <a target="_blank_" href="http://www.miit.gov.cn/" style="color:#c1c1c1;">豫ICP备19037971-2号</a>
            <!--            <a target="_blank">-->
            <!--                <img src="/static/home/images/beian.png" style="padding-bottom:6px">-->
            <!--                <span style="color:#c1c1c1;">豫公网安备 41152802000091号</span>-->
            <!--            </a>-->
            <span id="cnzz_stat_icon_1279280889">
            <a href="https://www.cnzz.com/stat/website.php?web_id=1279280889" target="_blank" title="站长统计">
              <img border="0" hspace="0" vspace="0" src="https://icon.cnzz.com/img/pic.gif">
            </a>
          </span>
        </div>
    </div>
</div>

        </footer>

        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="../js/main.js" defer></script>
        <script src="../js/gotop.js" defer></script>
        <script src="../js/baiducount.js" defer></script>
<!--    <script async src="../js/somelib.js"></script>-->
<!--<script data-ad-client="ca-pub-6937898095875663" async-->
<!--        src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>-->
<script data-ad-client="ca-pub-6937898095875663"
        src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
    //baidu自动收录
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>
<script>
    //360自动收录
    (function () {
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
    })();
</script>


        <div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
